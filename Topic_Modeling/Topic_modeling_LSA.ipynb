{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8c27193",
   "metadata": {},
   "source": [
    "# LSA\n",
    "    \n",
    "토픽 모델링을 위해 최적화 된 알고리즘은 아니다.\n",
    "\n",
    "BoW에 기반한 DTM, TF-IDF는 단어의 빈도수를 이용하기 떄문에 단어의 토픽을 고려하지 못한다.\n",
    "\n",
    "이를 위해 DTM의 잠재된 의미를 이끌어내는 방법으로 잠재 의미 분석(LSA) 방법으로 극복한다.\n",
    "\n",
    "선형대수학의 특이값 분해(SVD)의 원리를 사용한다.\n",
    "\n",
    "(DTM: 다수의 문자 데이터에 등장한 모든 단어의 빈도수를 행렬로 표현한 것)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04ea2f0",
   "metadata": {},
   "source": [
    "## 특이값 분해(Singular Calue Decomposition, Full SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d1a390",
   "metadata": {},
   "source": [
    "## 절단된 SVD\n",
    "\n",
    "절단된 SVD는 대각 행렬의 값중 상위 t개만 남고 이를 통해 손실이 일어나 기존의 행렬 A를 복구할 수 없다.\n",
    "\n",
    "또한 U,V행렬의 t열까지만 남기게 된다. 여기서 t는 우리가 찾고자 하는 토픽의 수를 반영한 하이퍼파라미터이다.\n",
    "\n",
    "(t를 크게 잡으면 다양한 의미를 A로부터 뽑을 수 있지만, t를 작게 잡아야 노이즈 제거 가능하다)\n",
    "\n",
    "t를 남겨 일부 벡터를 삭제하는 것을 데이터의 차원을 줄인다고 하는데, 이를 통해 계산 비용이 낮아지고 상대적으로 중요하지 않은 정보를 삭제 가능하다. 즉, 기존의 행렬에서 보지 못한 숨겨진 의미를 찾을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4565bfa2",
   "metadata": {},
   "source": [
    "## 잠재 의미 분석(LSA)\n",
    "LSA는 DTM이나 TF-IDF 행렬에 절단된 SVD를 사용해 차원을 축소시키고, 단어들의 잠재적 의미 이끌어낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a2312d",
   "metadata": {},
   "source": [
    "### Full SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "625f91b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTM의 크기(shape) : (4, 9)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[0,0,0,1,0,1,1,0,0],[0,0,0,1,1,0,1,0,0],[0,1,1,0,2,0,0,0,0],[1,0,0,0,0,0,0,1,1]])\n",
    "print('DTM의 크기(shape) :', np.shape(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e67330",
   "metadata": {},
   "source": [
    "4x9의 크기를 가지는 DTM 생성 후 full SVD 수행해보자.\n",
    "\n",
    "- S: 대각 행렬의 변수명\n",
    "- VT: V의 전치행렬\n",
    "- 소수점 둘째짜리까지만 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6dedaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "행렬 U :\n",
      "[[-0.24  0.75  0.   -0.62]\n",
      " [-0.51  0.44 -0.    0.74]\n",
      " [-0.83 -0.49 -0.   -0.27]\n",
      " [-0.   -0.    1.    0.  ]]\n",
      "행렬 U의 크기(shape) : (4, 4)\n"
     ]
    }
   ],
   "source": [
    "U, s, VT = np.linalg.svd(A, full_matrices = True)\n",
    "print('행렬 U :')\n",
    "print(U.round(2))\n",
    "print('행렬 U의 크기(shape) :',np.shape(U))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62f5f6d",
   "metadata": {},
   "source": [
    "대각행렬 S 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2598f123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특이값 벡터 :\n",
      "[2.69 2.05 1.73 0.77]\n",
      "특이값 벡터의 크기(shape) : (4,)\n"
     ]
    }
   ],
   "source": [
    "print('특이값 벡터 :')\n",
    "print(s.round(2))\n",
    "print('특이값 벡터의 크기(shape) :',np.shape(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4df52816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대각 행렬 S :\n",
      "[[2.69 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   2.05 0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   1.73 0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.77 0.   0.   0.   0.   0.  ]]\n",
      "대각 행렬의 크기(shape) :\n",
      "(4, 9)\n"
     ]
    }
   ],
   "source": [
    "#S를 다시 대각행렬로 변경해보자.\n",
    "\n",
    "# 대각 행렬의 크기인 4 x 9의 임의의 행렬 생성\n",
    "S = np.zeros((4, 9))\n",
    "\n",
    "# 특이값을 대각행렬에 삽입\n",
    "S[:4, :4] = np.diag(s)\n",
    "\n",
    "print('대각 행렬 S :')\n",
    "print(S.round(2))\n",
    "\n",
    "print('대각 행렬의 크기(shape) :')\n",
    "print(np.shape(S))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946f611e",
   "metadata": {},
   "source": [
    "위 대각행렬 S에서 값이 내림차순을 보인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9209aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "직교행렬 VT :\n",
      "[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\n",
      " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]\n",
      " [ 0.58 -0.    0.    0.   -0.    0.   -0.    0.58  0.58]\n",
      " [ 0.   -0.35 -0.35  0.16  0.25 -0.8   0.16 -0.   -0.  ]\n",
      " [-0.   -0.78 -0.01 -0.2   0.4   0.4  -0.2   0.    0.  ]\n",
      " [-0.29  0.31 -0.78 -0.24  0.23  0.23  0.01  0.14  0.14]\n",
      " [-0.29 -0.1   0.26 -0.59 -0.08 -0.08  0.66  0.14  0.14]\n",
      " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19  0.75 -0.25]\n",
      " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19 -0.25  0.75]]\n",
      "직교 행렬 VT의 크기(shape) :\n",
      "(9, 9)\n"
     ]
    }
   ],
   "source": [
    "print('직교행렬 VT :')\n",
    "print(VT.round(2))\n",
    "\n",
    "print('직교 행렬 VT의 크기(shape) :')\n",
    "print(np.shape(VT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46b1b85",
   "metadata": {},
   "source": [
    "9x9의 직교행렬 생성했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cabdc961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(A, np.dot(np.dot(U,S), VT).round(2))\n",
    "#기존의 행렬 A와 UxSxVT 값은 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362f2b8a",
   "metadata": {},
   "source": [
    "### Truncated SVD\n",
    "t = 2로 하고 Truncated SVD 수행해보자.\n",
    "\n",
    "즉, S 내의 특이값 중 상위 2개만 남긴다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7688186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대각 행렬 S :\n",
      "[[2.69 0.  ]\n",
      " [0.   2.05]]\n"
     ]
    }
   ],
   "source": [
    "# 특이값 상위 2개만 보존\n",
    "S = S[:2,:2]\n",
    "\n",
    "print('대각 행렬 S :')\n",
    "print(S.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01403044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "행렬 U :\n",
      "[[-0.24  0.75]\n",
      " [-0.51  0.44]\n",
      " [-0.83 -0.49]\n",
      " [-0.   -0.  ]]\n"
     ]
    }
   ],
   "source": [
    "#직교행렬 U에서도 2개의 열만 남기고 제거\n",
    "\n",
    "U = U[:,:2]\n",
    "print('행렬 U :')\n",
    "print(U.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6e9a2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "직교행렬 VT :\n",
      "[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\n",
      " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# VT도 마찬가지\n",
    "VT = VT[:2,:]\n",
    "print('직교행렬 VT :')\n",
    "print(VT.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a62efb",
   "metadata": {},
   "source": [
    "UxSxVT 연산을 해서 나오는 값을 A_prime이라 하고 A와 비교해보자.\n",
    "\n",
    "A_prime은 정보의 손실이 일어나 완벽하게는 복구할 수 없을것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "898a904c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 1 1 0 0]\n",
      " [0 0 0 1 1 0 1 0 0]\n",
      " [0 1 1 0 2 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 1]]\n",
      "[[ 0.   -0.17 -0.17  1.08  0.12  0.62  1.08 -0.   -0.  ]\n",
      " [ 0.    0.2   0.2   0.91  0.86  0.45  0.91  0.    0.  ]\n",
      " [ 0.    0.93  0.93  0.03  2.05 -0.17  0.03  0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "A_prime = np.dot(np.dot(U,S), VT)\n",
    "print(A)\n",
    "print(A_prime.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46b4c0a",
   "metadata": {},
   "source": [
    "- 기존의 0인 값은 0에 가깝게 나오고 1인 값은 1에 가깝게 나온다.\n",
    "- 축소된 U는 4x2의 값을 가지는데 이는 문서의 개수 x 토픽의 수 t이다. 이는 4개의 문서 각각을 2개의 값으로 표현한다고 할 수 있다.\n",
    "- 즉, U의 각 행은 잠재 의미를 표현하기 위한 수치화 된 각각의 문서 벡터이다.\n",
    "- 같은 의미로 VT의 각 열은 잠재 의미를 표현하기 위해 수치화된 각각의 단어 벡터라고 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cae6cd",
   "metadata": {},
   "source": [
    "## 실습\n",
    "\n",
    "Twenty Newsgroups의 20개 다른 주제를 가진 뉴스그룹 데이터에서 LSA를 활용해 문서의 수를 원하는 토픽의 수로 압축한 뒤 각 토픽당 가장 중요한 단어 5개를 출력해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dd322a",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a9bfb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfe5ed16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플의 수 : 11314\n"
     ]
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "print('샘플의 수 :',len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61e0da80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae78ef9c",
   "metadata": {},
   "source": [
    "위 뉴스그룹 데이터에서 원래 20개의 카테고리를 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "292ca7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940d34da",
   "metadata": {},
   "source": [
    "### 텍스트 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6516226e",
   "metadata": {},
   "source": [
    "알파벳을 제외한 문자 제거하고 짧은 단어 삭제, 그리고 알파벳을 소문자로 바꿔 단어의 개수 줄인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9decc742",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "# 특수 문자 제거\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "# 전체 단어에 대한 소문자 변환\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "177cf899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yeah expect people read actually accept hard atheism need little leap faith jimmy your logic runs steam sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway maybe start newsgroup atheist hard bummin much forget your flintstone chewables bake timmons'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['clean_doc'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65398c88",
   "metadata": {},
   "source": [
    "다음으로는 불용어 제거한다. 토큰화 후 불용어 제거."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b843e4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jihyeonbin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# NLTK로부터 불용어를 받아온다.\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) # 토큰화\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "# 불용어를 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb48ac94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yeah', 'expect', 'people', 'read', 'actually', 'accept', 'hard', 'atheism', 'need', 'little', 'leap', 'faith', 'jimmy', 'logic', 'runs', 'steam', 'sorry', 'pity', 'sorry', 'feelings', 'denial', 'faith', 'need', 'well', 'pretend', 'happily', 'ever', 'anyway', 'maybe', 'start', 'newsgroup', 'atheist', 'hard', 'bummin', 'much', 'forget', 'flintstone', 'chewables', 'bake', 'timmons']\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 완료\n",
    "print(tokenized_doc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d8bf0d",
   "metadata": {},
   "source": [
    "### TF-IDF 행렬 만들기\n",
    "\n",
    "TF-IDF는 토큰화가 되어있지 않아야해 역토큰화 진행."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "479b0eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenized_doc = []\n",
    "for i in range(len(news_df)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "news_df['clean_doc'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb340f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yeah expect people read actually accept hard atheism need little leap faith jimmy logic runs steam sorry pity sorry feelings denial faith need well pretend happily ever anyway maybe start newsgroup atheist hard bummin much forget flintstone chewables bake timmons'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['clean_doc'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2449f6ea",
   "metadata": {},
   "source": [
    "단어 1000개에 대한 TF-IDF 행렬을 TfidfVecotrizer를 통해 만들어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5285ebb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF 행렬의 크기 : (11314, 1000)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000, # 상위 1,000개의 단어를 보존 \n",
    "max_df = 0.5, smooth_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
    "\n",
    "# TF-IDF 행렬의 크기 확인\n",
    "print('TF-IDF 행렬의 크기 :',X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b02833",
   "metadata": {},
   "source": [
    "### 토픽 모델링\n",
    "\n",
    "TF-IDF 행렬을 Tuncated SVD를 통해 다수의 행렬로 분해 해보자. 20개의 토픽을 가졌다고 생각하고 모델링 시도한다.\n",
    "\n",
    "토픽의 숫자: n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9d0aa5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n",
    "svd_model.fit(X)\n",
    "len(svd_model.components_)\n",
    "\n",
    "#svd_model.components_: LSA에서의 VT, V의 직교행렬을 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05aeee6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(svd_model.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e25d00e",
   "metadata": {},
   "source": [
    "토픽의 수 x 단어의 수를 가진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94a17781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('like', 0.21386), ('know', 0.20046), ('people', 0.19293), ('think', 0.17805), ('good', 0.15128)]\n",
      "Topic 2: [('thanks', 0.32888), ('windows', 0.29088), ('card', 0.18069), ('drive', 0.17455), ('mail', 0.15111)]\n",
      "Topic 3: [('game', 0.37064), ('team', 0.32443), ('year', 0.28154), ('games', 0.2537), ('season', 0.18419)]\n",
      "Topic 4: [('drive', 0.53324), ('scsi', 0.20165), ('hard', 0.15628), ('disk', 0.15578), ('card', 0.13994)]\n",
      "Topic 5: [('windows', 0.40399), ('file', 0.25436), ('window', 0.18044), ('files', 0.16078), ('program', 0.13894)]\n",
      "Topic 6: [('chip', 0.16114), ('government', 0.16009), ('mail', 0.15625), ('space', 0.1507), ('information', 0.13562)]\n",
      "Topic 7: [('like', 0.67086), ('bike', 0.14236), ('chip', 0.11169), ('know', 0.11139), ('sounds', 0.10371)]\n",
      "Topic 8: [('card', 0.46633), ('video', 0.22137), ('sale', 0.21266), ('monitor', 0.15463), ('offer', 0.14643)]\n",
      "Topic 9: [('know', 0.46047), ('card', 0.33605), ('chip', 0.17558), ('government', 0.1522), ('video', 0.14356)]\n",
      "Topic 10: [('good', 0.42756), ('know', 0.23039), ('time', 0.1882), ('bike', 0.11406), ('jesus', 0.09027)]\n",
      "Topic 11: [('think', 0.78469), ('chip', 0.10899), ('good', 0.10635), ('thanks', 0.09123), ('clipper', 0.07946)]\n",
      "Topic 12: [('thanks', 0.36824), ('good', 0.22729), ('right', 0.21559), ('bike', 0.21037), ('problem', 0.20894)]\n",
      "Topic 13: [('good', 0.36212), ('people', 0.33985), ('windows', 0.28385), ('know', 0.26232), ('file', 0.18422)]\n",
      "Topic 14: [('space', 0.39946), ('think', 0.23258), ('know', 0.18074), ('nasa', 0.15174), ('problem', 0.12957)]\n",
      "Topic 15: [('space', 0.31613), ('good', 0.3094), ('card', 0.22603), ('people', 0.17476), ('time', 0.14496)]\n",
      "Topic 16: [('people', 0.48156), ('problem', 0.19961), ('window', 0.15281), ('time', 0.14664), ('game', 0.12871)]\n",
      "Topic 17: [('time', 0.34465), ('bike', 0.27303), ('right', 0.25557), ('windows', 0.1997), ('file', 0.19118)]\n",
      "Topic 18: [('time', 0.5973), ('problem', 0.15504), ('file', 0.14956), ('think', 0.12847), ('israel', 0.10903)]\n",
      "Topic 19: [('file', 0.44163), ('need', 0.26633), ('card', 0.18388), ('files', 0.17453), ('right', 0.15448)]\n",
      "Topic 20: [('problem', 0.33006), ('file', 0.27651), ('thanks', 0.23578), ('used', 0.19206), ('space', 0.13185)]\n"
     ]
    }
   ],
   "source": [
    "#각 20개의 행의 각 1000개 열 중 가장 값이 큰 5개의 값 단어 찾아서 출력\n",
    "\n",
    "terms = vectorizer.get_feature_names() # 단어 집합. 1,000개의 단어가 저장됨.\n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "get_topics(svd_model.components_,terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2103334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
